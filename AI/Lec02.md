# ğŸ§  What is an Intelligent Agent?

An **agent** = anything that perceives its environment through **sensors** and acts upon it with **actuators**.  

An **intelligent agent** = one that **chooses actions rationally** to achieve its goals (best possible outcome given what it knows).  

---

## ğŸ’¡ Analogy: Student as an Agent
- **Sensors**: eyes, ears â†’ see slides, hear lecture  
- **Actuators**: hands, mouth â†’ write notes, ask questions  
- **Goal**: pass the course with good grades  

---

# ğŸŒ Types of Agent Environments

According to **Russell & Norvig**, environments are classified using several dimensions:  

---

## 1. Observable vs. Partially Observable
- **Fully Observable**: Agent has access to the complete state of the environment.  
  - *Example*: **Chess** â†’ the board is fully visible.  
  - *Analogy*: Open-book exam â†’ you see all the information.  
- **Partially Observable**: Agent only has limited information.  
  - *Example*: **Poker** â†’ you canâ€™t see the opponentâ€™s cards.  
  - *Analogy*: Closed-book exam â†’ rely on memory, not all info is visible.  

---

## 2. Deterministic vs. Stochastic
- **Deterministic**: Next state is fully determined by current state + action.  
  - *Example*: Solving a math equation.  
  - *Analogy*: Following a recipe â†’ exact ingredients = exact result.  
- **Stochastic**: Involves randomness or uncertainty in outcomes.  
  - *Example*: **Self-driving car in traffic** â†’ canâ€™t predict pedestrians perfectly.  
  - *Analogy*: Playing football â†’ strategy helps, but outcomes are uncertain.  

---

## 3. Episodic vs. Sequential
- **Episodic**: Each action is independent of the previous one.  
  - *Example*: Image classification â†’ each picture is separate.  
  - *Analogy*: Quiz show â†’ each question is unrelated.  
- **Sequential**: Current decision affects future decisions.  
  - *Example*: Driving a car â†’ each turn affects destination.  
  - *Analogy*: Choosing university courses â†’ affects prerequisites and graduation path.  

---

## 4. Static vs. Dynamic
- **Static**: Environment doesnâ€™t change while the agent is deciding.  
  - *Example*: Crossword puzzle â†’ grid doesnâ€™t change.  
  - *Analogy*: Paused game â†’ time frozen while you think.  
- **Dynamic**: Environment changes during decision-making.  
  - *Example*: Driving in traffic â†’ cars keep moving.  
  - *Analogy*: Group discussion â†’ people add points as you think.  

---

## 5. Discrete vs. Continuous
- **Discrete**: Finite set of states, actions, or percepts.  
  - *Example*: Chess â†’ 64 squares, fixed moves.  
  - *Analogy*: Multiple-choice exam â†’ limited answers.  
- **Continuous**: Infinite possible states or actions.  
  - *Example*: Driving â†’ continuous speed and steering.  
  - *Analogy*: Essay exam â†’ endless possible answers.  

---

## ğŸ“Š Comparison Table

| **Property**     | **Discrete Example**                | **Continuous Example**            |
|------------------|-------------------------------------|-----------------------------------|
| **Observable**   | Chess (see everything)              | Poker (hidden cards)              |
| **Deterministic**| Solving equations                   | Weather prediction (uncertain)    |
| **Episodic**     | Spam filter (each email separate)   | Self-driving car (turns affect future) |
| **Static**       | Sudoku puzzle                       | Stock market (changes while you think) |
| **Discrete**     | Tic-tac-toe, multiple-choice exam   | Driving, handwriting recognition  |

---

## âœ… Key Idea
The **design of an intelligent agent** depends on its environment:  
- **Chess-playing agent** â†’ needs **strategy & search**.  
- **Self-driving car** â†’ needs **perception, uncertainty handling, and real-time decision-making**.  
